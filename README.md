# SERUM @ WACV 2023
### *SE*mantic Data Engineering for *R*obustness *U*nder *M*ultimodal Settings
---
Website for SERUM Tutorial at WACV 2023, *January 7, 2PM to 5PM* 

Hosted by [Tejas Gokhale](https://www.tejasgokhale.com/) and [Yezhou Yang](https://yezhouyang.engineering.asu.edu/) (Arizona State University)

## Description
In the past decade, we have witnessed a paradigm shift in computer vision -- the connection between vision and language (V+L) is now an integral part of AI.
V+L comprises of human-interactive tasks such as visual question answering, image captioning, visual dialog, visual entailment and grounding, V+L navigation, and text-to-image generation.
This field has already had an impact on other research communities such as NLP, robotics, graphics, and direct industrial implications for software, arts, media, and journalism.
As V+L models become widely adopted, new types of challenges and failure modes are emerging, that have not been studied by previous work on robustness.
Multi-modal tasks involving both vision and language (V+L) inputs, open up intriguing domain discrepancies that can affect model performance of test time.

In this tutorial, we will show how semantic data transformation -- i.e. data transformation guided by the knowledge of logical and semantic features of natural language, can 
- help improve the robustness of V+L models,
- enable weakly supervised learning in cases with limited or no human-annotated datasets,
- enhance the quality of outputs in generative settings such as captioning, and 
- guide multi-modal knowledge retrieval for knowledge-based visual question answering.

## Tentative Schedule
<table>
	<tr>
		<th> Time </th>
		<th> Topic </th>
		<th> Presenter </th>
	</tr>
	<tr>
		<td> 1400--1415 </td>
		<td> Welcome and Introduction </td>
		<td valign="center"> 
			<img style="float: left;" src="https://test-fac-yezhou-yang.pantheonsite.io/wp-content/uploads/2018/07/CIDSE-Yezhou-Yang-Lab-MAC0089a-small.jpg" width="30%"/>   
			<a href="">Yezhou Yang</a> <br/>
			(Associate Professor, ASU)
		</td>
	</tr>
	<tr>
		<td> 1415--1515 </td>
		<td> Recent Advances in Multimodal Foundation Models </td>
		<td valign="center"> 
			<img src="https://zhegan27.github.io/images/Zhe_new.jpg" width="30%"/>
			<a href="">Zhe Gan</a> <br/>
			(Staff Research Scientist, Apple)
		</td>
	</tr>
</table>


| Time                | Topic                                        | Presenter   |
| ------------------- | -------------------------------------------- | ----------- |
| 1400--1415     | Welcome and Introduction  | <img style="float: left;" src="https://test-fac-yezhou-yang.pantheonsite.io/wp-content/uploads/2018/07/CIDSE-Yezhou-Yang-Lab-MAC0089a-small.jpg" width="30%"/>   <div align="center"><a href="">Yezhou Yang</a> (Associate Professor, ASU) |
| 1415--1515     | Recent Advances in Multimodal Foundation Models | <img src="https://zhegan27.github.io/images/Zhe_new.jpg" width="30%"/>    <a href="">Zhe Gan</a> (Staff Research Scientist, Apple) |
| 1515--1600     | Robustness via Knowledge-Guided Data Augmentation and Adversarial Training | <img src="https://www.tejasgokhale.com/images/tg_brickyard.jpg" width="30%"/>    <a href="">Tejas Gokhale</a> (Ph.D. Candidate, ASU) |
| 1600--1620     | Enhancing Video Captioning with Commonsense Descriptions | <img src="https://test-fac-yezhou-yang.pantheonsite.io/wp-content/uploads/2018/07/CIDSE-Yezhou-Yang-Lab-MAC0089a-small.jpg" width="30%"/>    <a href="">Yezhou Yang</a> (Associate Professor, ASU) |
| 1620--1645     | Visual-Retriever-Reader for Knowledge-based Question Answering | <img src="https://luomancs.github.io/images/manluo.jpeg" width="30%"/>    <a href="">Man Luo</a> (Ph.D. Candidate, ASU) |
| 1645--1700     | Concluding Remarks | <img src="https://www.tejasgokhale.com/images/tg_brickyard.jpg" width="30%"/>Tejas Gokhale (Ph.D. Candidate, ASU) |



---
*This website will be updated closer to the event date.*
