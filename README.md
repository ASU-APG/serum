# SERUM @ WACV 2023
### *SE*mantic Data Engineering for *R*obustness *U*nder *M*ultimodal Settings
---
Website for SERUM Tutorial at WACV 2023, *January 7, 2PM to 5PM* 

Hosted by [Tejas Gokhale](https://www.tejasgokhale.com/) and [Yezhou Yang](https://yezhouyang.engineering.asu.edu/) (Arizona State University)

## Agenda
In the past decade, we have witnessed a paradigm shift in computer vision -- the connection between vision and language (V+L) is now an integral part of AI.
V+L comprises of human-interactive tasks such as visual question answering, image captioning, visual dialog, visual entailment and grounding, V+L navigation, and text-to-image generation.
This field has already had an impact on other research communities such as NLP, robotics, graphics, and direct industrial implications for software, arts, media, and journalism.
As V+L models become widely adopted, new types of challenges and failure modes are emerging, that have not been studied by previous work on robustness.
Multi-modal tasks involving both vision and language (V+L) inputs, open up intriguing domain discrepancies that can affect model performance of test time.

In this tutorial, we will show how semantic data transformation -- i.e. data transformation guided by the knowledge of logical and semantic features of natural language, can 
- help improve the robustness of V+L models,
- enable weakly supervised learning in cases with limited or no human-annotated datasets,
- enhance the quality of outputs in generative settings such as captioning, and 
- guide multi-modal knowledge retrieval for knowledge-based visual question answering.

## Tentative Schedule
<table>
	<tr>
		<th width="15%"> Time (UTC-10) </th>
		<th width="40%"> Topic </th>
		<th> Presenter </th>
	</tr>
	<tr>
		<td> 1400--1415 </td>
		<td> Welcome and Introduction </td>
		<td valign="center"> 
			<img style="padding-right: 5%; float: left;" src="https://test-fac-yezhou-yang.pantheonsite.io/wp-content/uploads/2018/07/CIDSE-Yezhou-Yang-Lab-MAC0089a-small.jpg" width="50%"/>   
			<a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a> <br/>(Associate Professor, ASU
		</td>
	</tr>
	<tr>
		<td> 1415--1515 </td>
		<td> Recent Advances in Multimodal Foundation Models </td>
		<td valign="center"> 
			<img style="padding-right: 5%; float: left;" src="https://zhegan27.github.io/images/Zhe_new.jpg" width="50%"/>
			<a href="https://zhegan27.github.io">Zhe Gan</a> <br/> (Staff Research Scientist, Apple
		</td>
	</tr>
	<tr>
		<td> 1515--1600 </td>
		<td> Robustness via Knowledge-Guided Data Augmentation and Adversarial Training </td>
		<td valign="center"> 
			<img style="padding-right: 5%; float: left;" src="https://www.tejasgokhale.com/images/tg_brickyard.jpg" width="50%"/>
			 <a href="https://www.tejasgokhale.com">Tejas Gokhale</a> <br/> (Ph.D. Candidate, ASU)
		</td>
	</tr>
	<tr>
		<td> 1600--1620 </td>
		<td> Enhancing Video Captioning with Commonsense Descriptions </td>
		<td valign="center">
			<img style="padding-right: 5%; float: left;" src="https://test-fac-yezhou-yang.pantheonsite.io/wp-content/uploads/2018/07/CIDSE-Yezhou-Yang-Lab-MAC0089a-small.jpg" width="50%"/>
			<a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a> <br/> (Associate Professor, ASU)
		</td>
	</tr>
	<tr>
		<td> 1620--1645 </td>
		<td> Visual-Retriever-Reader for Knowledge-based Question Answering </td>
		<td valign="center">
			<img style="padding-right: 5%; float: left;" src="https://luomancs.github.io/images/manluo.jpeg" width="50%"/>
			<a href="https://luomancs.github.io">Man Luo</a> <br/> (Ph.D. Candidate, ASU)
		</td>
	</tr>
	<tr>
		<td> 1645--1700 </td> 
		<td> Concluding Remarks</td>
		<td valign="center">
			<img style="padding-right: 5%; float: left;" src="https://www.tejasgokhale.com/images/tg_brickyard.jpg" width="50%"/>
			<a href="https://www.tejasgokhale.com">Tejas Gokhale</a> <br/> (Ph.D. Candidate, ASU)
		</td>
	</tr>
</table>



---
*This website will be updated closer to the event date.*


```
We acknowledge support from NSF Robust Intelligence grant #2132724
```
